import pandas as pd
from sklearn.metrics import accuracy_score, f1_score
# Correction de l'import : on prÃ©cise 'src.sentiment' pour que Ã§a marche depuis le dashboard
from src.sentiment import analyze_sentiment

# 1. CRÃ‰ATION DU JEU DE DONNÃ‰ES "VÃ‰RITÃ‰ TERRAIN"
donnees_test = [
    {"text": "J'adore ce produit, il est gÃ©nial !", "verite": "Positif ğŸ˜ƒ"},
    {"text": "C'est une catastrophe, je dÃ©teste.", "verite": "NÃ©gatif ğŸ˜¡"},
    {"text": "Livraison rapide et soignÃ©e.", "verite": "Positif ğŸ˜ƒ"},
    {"text": "Bof, Ã§a passe mais c'est cher.", "verite": "Neutre ğŸ˜"},
    {"text": "Le service client ne rÃ©pond jamais.", "verite": "NÃ©gatif ğŸ˜¡"},
    {"text": "Correct sans plus.", "verite": "Neutre ğŸ˜"},
    {"text": "Best purchase ever, I love it!", "verite": "Positif ğŸ˜ƒ"},
    {"text": "Very bad quality.", "verite": "NÃ©gatif ğŸ˜¡"}
]

# 2. FONCTION APPELÃ‰E PAR LE DASHBOARD
def get_metrics():
    """
    Fonction qui calcule l'Accuracy et le F1-Score.
    Retourne : (accuracy, f1_score, nombre_echantillons)
    """
    y_true = []
    y_pred = []
    
    # On boucle sur chaque phrase test
    for item in donnees_test:
        text = item["text"]
        realite = item["verite"]
        
        # On demande Ã  l'IA
        pred_label, score, _ = analyze_sentiment(text)
        
        y_true.append(realite)
        y_pred.append(pred_label)

    # Calcul des mÃ©triques
    accuracy = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average='weighted') 
    
    return accuracy, f1, len(donnees_test)

# Petit bloc pour tester ce fichier tout seul si besoin
if __name__ == "__main__":
    acc, f1, n = get_metrics()
    print(f"Test manuel : Accuracy={acc}, F1={f1}")